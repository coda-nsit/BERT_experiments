{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_word_embeddings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5Z5fC1Sr08fnZ6GKXU9Uv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coda-nsit/BERT_experiments/blob/master/BERT_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Bcg-Q3B44d",
        "colab_type": "text"
      },
      "source": [
        "# What does this notebook do?\n",
        "I tried to get all the word embeddings of words fed to the Bert model. I also changed one of the config parameters to output all the hidden layers.\n",
        "\n",
        "Reference: https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
        "The above reference doesn't use the Transformers library but the pytorch-pretrained-bert. I have modified it to use transformers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSIAvWW7uuTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "4dbcebf0-b2bd-45da-dd2d-f9c5ef6257a4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl (544kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 2.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.31)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.31)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=11857d29bd4f04aed0a5bd60f41b582aeb97db900ba20a6edd9f85e77502d0c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFIx_5kwu0Np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "% matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-Oh2zs5vXSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change logging to see everything that is output\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncn1p8FAwBKV",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VxN--0qvqi3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "16315a83-9ac3-43a0-b9d2-4f88a84bd140"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzWeFo8gw8wI",
        "colab_type": "text"
      },
      "source": [
        "## Sample tokenization\n",
        "`['em', '##bed', '##ding', '##s']`: The original word has been split into smaller subwords and characters. The two hash signs preceding some of these subwords are just our tokenizer’s way to denote that this subword or character is part of a larger word and preceded by another subword. So, for example, the ‘##bed’ token is separate from the ‘bed’ token; the first is used whenever the subword ‘bed’ occurs within a larger word and the second is used explicitly for when the standalone token ‘thing you sleep on’ occurs. \n",
        "\n",
        "This model greedily creates a fixed-size vocabulary of individual characters, subwords, and words that best fits our language data.\n",
        "\n",
        "https://colab.research.google.com/drive/1fCKIBJ6fgWQ-f6UKs7wDTpNTL9N-Cq9X: Notebook on BERT's vocabulary.\n",
        "https://youtu.be/zJW57aCBCTk: Video on BERT's vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j9aKZ44vtLn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "718f670d-9f86-4b79-dc9d-bfd90422ab04"
      },
      "source": [
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUnsKC-wzXBX",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the text for input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT19zztSxAye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "da739c03-d4e6-488c-8f68-3f68925fab69"
      },
      "source": [
        "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
        "       \"fishing on the Mississippi river bank.\"\n",
        "\n",
        "# 1. Add the special tokens\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# 2. Split the sentence into tokens\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# 3. Map the token strings to their vocabulary indeces\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# 4. Display the words with their indices.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
        "\n",
        "# 5. Segment IDs: 0 for sentence 1 and 1 for sentence 2\n",
        "segments_ids = [1] * len(tokenized_text)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "after         2,044\n",
            "stealing     11,065\n",
            "money         2,769\n",
            "from          2,013\n",
            "the           1,996\n",
            "bank          2,924\n",
            "vault        11,632\n",
            ",             1,010\n",
            "the           1,996\n",
            "bank          2,924\n",
            "robber       27,307\n",
            "was           2,001\n",
            "seen          2,464\n",
            "fishing       5,645\n",
            "on            2,006\n",
            "the           1,996\n",
            "mississippi   5,900\n",
            "river         2,314\n",
            "bank          2,924\n",
            ".             1,012\n",
            "[SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q32RYnY_0VHG",
        "colab_type": "text"
      },
      "source": [
        "## Convert the input to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnU61o8hzlN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJDDem_u0cOZ",
        "colab_type": "text"
      },
      "source": [
        "# Get the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBsSwmLW0dmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0b99f433-234d-4870-bb08-7d43b1fd28ab"
      },
      "source": [
        "# Initializing a BERT bert-base-uncased style configuration\n",
        "configuration = BertConfig()\n",
        "\n",
        "# changed the default config so that now the model also \n",
        "configuration.output_hidden_states = True\n",
        "\n",
        "# outputting raw hidden-states without any specific head on top\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\", config=configuration)\n",
        "model.eval()\n",
        "\n",
        "# stop the memorization of the gradients and get the model forward pass\n",
        "with torch.no_grad():\n",
        "  embeddings, cls, hidden_states = model(tokens_tensor, segments_tensors)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MBs03ax7XhY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3866309e-0ea6-4032-d099-931c4a45cde1"
      },
      "source": [
        "print(\"embeddings are:\")\n",
        "display(embeddings)\n",
        "print(\"\\n\")\n",
        "\n",
        "# (batch number, tokens, embedding vector size)\n",
        "print(\"embeddings shape:\", embeddings.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"input token shape:\", len(tokenized_text))\n",
        "print(\"\\n\")\n",
        "\n",
        "# 12 + 1: one for the output of the embeddings + one for the output of each layer\n",
        "# each layer of shape: (batch_size, sequence_length, embedding vector size)\n",
        "print(\"hidden state shape:\", len(hidden_states))\n",
        "print(\"hidden state layer 1:\", hidden_states[0].shape)\n",
        "print(\"hidden state layer 2:\", hidden_states[1].shape)\n",
        "print(\"hidden state layer 3:\", hidden_states[2].shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings are:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
              "         [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
              "         [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
              "         ...,\n",
              "         [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
              "         [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
              "         [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]]])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "embeddings shape: torch.Size([1, 22, 768])\n",
            "\n",
            "\n",
            "input token shape: 22\n",
            "\n",
            "\n",
            "hidden state shape: 13\n",
            "hidden state layer 1: torch.Size([1, 22, 768])\n",
            "hidden state layer 2: torch.Size([1, 22, 768])\n",
            "hidden state layer 3: torch.Size([1, 22, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}