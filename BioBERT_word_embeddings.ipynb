{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BioBERT_word_embeddings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPafApuR1eGuNQKvsDbNoIw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coda-nsit/BERT_experiments/blob/master/BioBERT_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEDuX_V3TauP",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "1. ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory: https://github.com/tensorflow/tensorflow/issues/15604\n",
        "2. Tensorflow model not found error while converting it to PyTorch model: https://github.com/tensorflow/models/issues/2676#issuecomment-421765031\n",
        "3. Install cuda-9-0: https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1604&target_type=debnetwork\n",
        "4. Tensorflow to PyTorch: https://huggingface.co/transformers/converting_tensorflow_models.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP8uHfU5Whzf",
        "colab_type": "text"
      },
      "source": [
        "# Install and download libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L3NLZdZImvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9e1cd9e-cf88-43eb-b55c-1a0b1567602c"
      },
      "source": [
        "!pip3 install transformers\n",
        "!git clone https://github.com/dmis-lab/biobert.git\n",
        "!pip3 install -r biobert/requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl (544kB)\n",
            "\r\u001b[K     |▋                               | 10kB 27.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 27.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 32.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 36.7MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 27.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 30.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 71kB 23.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 25.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 92kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 122kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 143kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 163kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 184kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 194kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 204kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 215kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 235kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 245kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 256kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 266kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 276kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 286kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 296kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 307kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 317kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 327kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 337kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 348kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 358kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 368kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 378kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 389kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 399kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 409kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 419kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 430kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 440kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 450kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 460kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 471kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 481kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 491kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 501kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 512kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 522kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 532kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 552kB 24.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 56.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 64.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.31)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=d9d46c2d6795a9e01cbec306166f8f6ca7c7965c8446c50218c64fa4088a5de1\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.7.0\n",
            "Cloning into 'biobert'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 293 (delta 7), reused 0 (delta 0), pack-reused 278\u001b[K\n",
            "Receiving objects: 100% (293/293), 489.78 KiB | 554.00 KiB/s, done.\n",
            "Resolving deltas: 100% (169/169), done.\n",
            "Collecting tensorflow-gpu==1.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/52/01438b81806765936eee690709edc2a975472c4e9d8d465a01840869c691/tensorflow_gpu-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (258.8MB)\n",
            "\u001b[K     |████████████████████████████████| 258.8MB 60kB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r biobert/requirements.txt (line 6)) (0.0)\n",
            "Collecting pandas==0.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ec/8ff0800b8594691759b78a42ccd616f81e7099ee47b167eb9bbd502c02b9/pandas-0.23.0-cp36-cp36m-manylinux1_x86_64.whl (11.7MB)\n",
            "\u001b[K     |████████████████████████████████| 11.7MB 64.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (1.1.0)\n",
            "Collecting tensorboard<1.12.0,>=1.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/2f/4d788919b1feef04624d63ed6ea45a49d1d1c834199ec50716edb5d310f4/tensorboard-1.11.0-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 67.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (0.34.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (1.27.2)\n",
            "Collecting setuptools<=39.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 61.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (1.18.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->-r biobert/requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23->-r biobert/requirements.txt (line 7)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23->-r biobert/requirements.txt (line 7)) (2018.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.5->tensorflow-gpu==1.11.0->-r biobert/requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r biobert/requirements.txt (line 6)) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r biobert/requirements.txt (line 6)) (1.4.1)\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement setuptools>=41.2, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-auth 1.7.2 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.6 has requirement pandas>=0.23.4, but you'll have pandas 0.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, setuptools, tensorflow-gpu, pandas\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: setuptools 46.0.0\n",
            "    Uninstalling setuptools-46.0.0:\n",
            "      Successfully uninstalled setuptools-46.0.0\n",
            "  Found existing installation: pandas 1.0.3\n",
            "    Uninstalling pandas-1.0.3:\n",
            "      Successfully uninstalled pandas-1.0.3\n",
            "Successfully installed pandas-0.23.0 setuptools-39.1.0 tensorboard-1.11.0 tensorflow-gpu-1.11.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas",
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc91JT7isU-J",
        "colab_type": "text"
      },
      "source": [
        "BioBERT requires Tensorflow==1.11, which uses Cuda 9.0, so uninstall the latest version and reinstall the old version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS149ONspSuY",
        "colab_type": "code",
        "outputId": "01a951d3-8679-4d7e-9942-a2b3d56257e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt remove cuda\n",
        "!apt-get purge nvidia-cuda*\n",
        "!apt autoremove\n",
        "!wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\n",
        "!apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt install cuda-9-0\n",
        "!rm cuda-repo-ubuntu1604_9.0.176-1_amd64.deb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'nvidia-cuda-toolkit' for glob 'nvidia-cuda*'\n",
            "Note, selecting 'nvidia-cuda-dev' for glob 'nvidia-cuda*'\n",
            "Note, selecting 'nvidia-cuda-doc' for glob 'nvidia-cuda*'\n",
            "Note, selecting 'nvidia-cuda-gdb' for glob 'nvidia-cuda*'\n",
            "Package 'nvidia-cuda-dev' is not installed, so not removed\n",
            "Package 'nvidia-cuda-doc' is not installed, so not removed\n",
            "Package 'nvidia-cuda-gdb' is not installed, so not removed\n",
            "Package 'nvidia-cuda-toolkit' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "--2020-04-02 00:48:48--  http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2838 (2.8K) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604_9.0.176-1_amd64.deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   2.77K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-04-02 00:48:48 (725 MB/s) - ‘cuda-repo-ubuntu1604_9.0.176-1_amd64.deb’ saved [2838/2838]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604.\n",
            "(Reading database ... 144542 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu1604 (9.0.176-1) ...\n",
            "Setting up cuda-repo-ubuntu1604 (9.0.176-1) ...\n",
            "\n",
            "Configuration file '/etc/apt/sources.list.d/cuda.list'\n",
            " ==> File on system created by you or by a script.\n",
            " ==> File also in package provided by package maintainer.\n",
            "   What would you like to do about it ?  Your options are:\n",
            "    Y or I  : install the package maintainer's version\n",
            "    N or O  : keep your currently-installed version\n",
            "      D     : show the differences between the versions\n",
            "      Z     : start a shell to examine the situation\n",
            " The default action is to keep your current version.\n",
            "*** cuda.list (Y/I/N/O/D/Z) [default=N] ? Y\n",
            "Installing new version of config file /etc/apt/sources.list.d/cuda.list ...\n",
            "Executing: /tmp/apt-key-gpghome.vCVyCTcqyt/gpg.1.sh --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\n",
            "gpg: requesting key from 'http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub'\n",
            "gpg: key F60F4B3D7FA2AF80: \"cudatools <cudatools@nvidia.com>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [87.7 kB]\n",
            "Ign:3 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  InRelease\n",
            "Get:4 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release [564 B]\n",
            "Get:5 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release.gpg [819 B]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Get:9 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Packages [254 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [31.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [37.0 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [870 kB]\n",
            "Get:18 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,790 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [835 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,161 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [50.4 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [12.2 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,367 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4,247 B]\n",
            "Get:26 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [863 kB]\n",
            "Fetched 7,637 kB in 7s (1,173 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "0 upgraded, 33 newly installed, 0 to remove and 61 not upgraded.\n",
            "Need to get 1,126 MB of archives.\n",
            "After this operation, 2,386 MB of additional disk space will be used.\n",
            "Get:1 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-license-9-0 9.0.176-1 [22.0 kB]\n",
            "Get:2 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-misc-headers-9-0 9.0.176-1 [684 kB]\n",
            "Get:3 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-core-9-0 9.0.176.3-1 [16.9 MB]\n",
            "Get:4 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cudart-9-0 9.0.176-1 [106 kB]\n",
            "Get:5 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-driver-dev-9-0 9.0.176-1 [10.9 kB]\n",
            "Get:6 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cudart-dev-9-0 9.0.176-1 [767 kB]\n",
            "Get:7 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-command-line-tools-9-0 9.0.176-1 [25.4 MB]\n",
            "Get:8 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-nvrtc-9-0 9.0.176-1 [6,348 kB]\n",
            "Get:9 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-nvrtc-dev-9-0 9.0.176-1 [9,334 B]\n",
            "Get:10 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cusolver-9-0 9.0.176-1 [26.2 MB]\n",
            "Get:11 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cusolver-dev-9-0 9.0.176-1 [5,317 kB]\n",
            "Get:12 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cublas-9-0 9.0.176.4-1 [52.1 MB]\n",
            "Get:13 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cublas-dev-9-0 9.0.176.4-1 [51.3 MB]\n",
            "Get:14 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cufft-9-0 9.0.176-1 [84.1 MB]\n",
            "Get:15 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cufft-dev-9-0 9.0.176-1 [73.7 MB]\n",
            "Get:16 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-curand-9-0 9.0.176-1 [38.8 MB]\n",
            "Get:17 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-curand-dev-9-0 9.0.176-1 [57.9 MB]\n",
            "Get:18 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cusparse-9-0 9.0.176-1 [25.2 MB]\n",
            "Get:19 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-cusparse-dev-9-0 9.0.176-1 [25.3 MB]\n",
            "Get:20 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-npp-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:21 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-npp-dev-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:22 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-nvgraph-9-0 9.0.176-1 [6,081 kB]\n",
            "Get:23 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-nvgraph-dev-9-0 9.0.176-1 [5,658 kB]\n",
            "Get:24 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-samples-9-0 9.0.176-1 [75.9 MB]\n",
            "Get:25 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-documentation-9-0 9.0.176-1 [53.1 MB]\n",
            "Get:26 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-libraries-dev-9-0 9.0.176-1 [2,596 B]\n",
            "Get:27 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-nvml-dev-9-0 9.0.176-1 [47.6 kB]\n",
            "Get:28 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-visual-tools-9-0 9.0.176-1 [398 MB]\n",
            "Get:29 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-toolkit-9-0 9.0.176-1 [2,836 B]\n",
            "Get:30 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-libraries-9-0 9.0.176-1 [2,566 B]\n",
            "Get:31 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-runtime-9-0 9.0.176-1 [2,526 B]\n",
            "Get:32 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-demo-suite-9-0 9.0.176-1 [3,880 kB]\n",
            "Get:33 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  cuda-9-0 9.0.176-1 [2,552 B]\n",
            "Fetched 1,126 MB in 25s (45.5 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package cuda-license-9-0.\n",
            "(Reading database ... 144545 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-license-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-9-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-core-9-0.\n",
            "Preparing to unpack .../02-cuda-core-9-0_9.0.176.3-1_amd64.deb ...\n",
            "Unpacking cuda-core-9-0 (9.0.176.3-1) ...\n",
            "Selecting previously unselected package cuda-cudart-9-0.\n",
            "Preparing to unpack .../03-cuda-cudart-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-9-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-9-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-9-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-9-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-9-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-9-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-9-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-9-0.\n",
            "Preparing to unpack .../11-cuda-cublas-9-0_9.0.176.4-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-9-0 (9.0.176.4-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-9-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-9-0_9.0.176.4-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-9-0 (9.0.176.4-1) ...\n",
            "Selecting previously unselected package cuda-cufft-9-0.\n",
            "Preparing to unpack .../13-cuda-cufft-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-9-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-9-0.\n",
            "Preparing to unpack .../15-cuda-curand-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-9-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-9-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-9-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-9-0.\n",
            "Preparing to unpack .../19-cuda-npp-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-9-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-9-0.\n",
            "Preparing to unpack .../21-cuda-nvgraph-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-9-0.\n",
            "Preparing to unpack .../22-cuda-nvgraph-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-samples-9-0.\n",
            "Preparing to unpack .../23-cuda-samples-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-samples-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-documentation-9-0.\n",
            "Preparing to unpack .../24-cuda-documentation-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-9-0.\n",
            "Preparing to unpack .../25-cuda-libraries-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-9-0.\n",
            "Preparing to unpack .../26-cuda-nvml-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-9-0.\n",
            "Preparing to unpack .../27-cuda-visual-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-9-0.\n",
            "Preparing to unpack .../28-cuda-toolkit-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-9-0.\n",
            "Preparing to unpack .../29-cuda-libraries-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-runtime-9-0.\n",
            "Preparing to unpack .../30-cuda-runtime-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-9-0.\n",
            "Preparing to unpack .../31-cuda-demo-suite-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-9-0.\n",
            "Preparing to unpack .../32-cuda-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-license-9-0 (9.0.176-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-9.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-9-0 (9.0.176.4-1) ...\n",
            "Setting up cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-core-9-0 (9.0.176.3-1) ...\n",
            "Setting up cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-dev-9-0 (9.0.176.4-1) ...\n",
            "Setting up cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-samples-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-9-0 (9.0.176-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4I3oNPqXbtL",
        "colab_type": "text"
      },
      "source": [
        "### Load the BioBERT weights \n",
        "The weights are saved in google drive. Not using `wget` cause, the weights are saved in google drive, downloading it is difficult."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewC1AlWvXcYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6013541e-94ae-4498-aa2b-446b63e047c3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExtcVb3AXmbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /gdrive/\"My Drive\"/OncampusJob/biobert_v1.1_pubmed.tar.gz .\n",
        "!tar -xzf biobert_v1.1_pubmed.tar.gz\n",
        "!rm biobert_v1.1_pubmed.tar.gz\n",
        "%mv  biobert_v1.1_pubmed biobert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbGGMDbbw2kO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d7893eb-ee3c-4294-817a-a0a1636cbbaa"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_base  biobert  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcZRyl8JalWa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fbd630a5-5676-4b0d-ef8c-1e5fb607852f"
      },
      "source": [
        "%env BIO_BERT_BASE_DIR=/content/biobert\n",
        "!transformers-cli convert \\\n",
        "  --model_type bert \\\n",
        "  --tf_checkpoint $BIO_BERT_BASE_DIR/model.ckpt-1000000 \\\n",
        "  --config $BIO_BERT_BASE_DIR/bert_config.json \\\n",
        "  --pytorch_dump_output $BIO_BERT_BASE_DIR/pytorch_model.bin"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: BIO_BERT_BASE_DIR=/content/biobert\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": null,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_bert:Converting TensorFlow checkpoint from /content/biobert/model.ckpt-1000000\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/word_embeddings with shape [28996, 768]\n",
            "2020-04-02 02:43:43.695540: W tensorflow/core/framework/allocator.cc:113] Allocation of 89075712 exceeds 10% of system memory.\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Save PyTorch model to /content/biobert/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI-jIVC9b7AQ",
        "colab_type": "text"
      },
      "source": [
        "### Load the Tensorflow BERT weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8EmkbVddF1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "206bd8b3-d749-472d-bdca-a21d2e703beb"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
        "!unzip cased_L-12_H-768_A-12.zip\n",
        "!rm cased_L-12_H-768_A-12.zip\n",
        "!mv cased_L-12_H-768_A-12 bert_base"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-02 01:58:19--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.194.128, 2404:6800:4003:c03::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.194.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 404261442 (386M) [application/zip]\n",
            "Saving to: ‘cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 385.53M  86.6MB/s    in 4.5s    \n",
            "\n",
            "2020-04-02 01:58:24 (86.6 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [404261442/404261442]\n",
            "\n",
            "Archive:  cased_L-12_H-768_A-12.zip\n",
            "   creating: cased_L-12_H-768_A-12/\n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEsc9j4NdyfU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "edd011b8-8c87-4f7e-a777-7c307a10c3c5"
      },
      "source": [
        "!ls bert_base/"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t     bert_model.ckpt.index  pytorch_model.bin\n",
            "bert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta   vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgXzzxiYN0Vf",
        "colab_type": "code",
        "outputId": "294c08b0-4bb1-4256-c9f7-b638358a7887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%env BERT_BASE_DIR=/content/bert_base\n",
        "!transformers-cli convert \\\n",
        "  --model_type bert \\\n",
        "  --tf_checkpoint $BERT_BASE_DIR/bert_model.ckpt \\\n",
        "  --config $BERT_BASE_DIR/bert_config.json \\\n",
        "  --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: BERT_BASE_DIR=/content/bert_base\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": null,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_bert:Converting TensorFlow checkpoint from /content/bert_base/bert_model.ckpt\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/word_embeddings with shape [28996, 768]\n",
            "2020-04-02 01:59:36.666589: W tensorflow/core/framework/allocator.cc:113] Allocation of 89075712 exceeds 10% of system memory.\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/output_bias with shape [28996]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
            "Save PyTorch model to /content/bert_base/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PlIZDePyuEL",
        "colab_type": "text"
      },
      "source": [
        "# Get the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikM5UD4fnxb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12d0f2d1-8d66-46a2-98a9-2e5ccbbb8c93"
      },
      "source": [
        "!ls "
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_base  biobert  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyxxF8iry7BQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d36885cc-1bfc-4276-f28b-9493c2dad0ae"
      },
      "source": [
        "!ls biobert/"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
            "model.ckpt-1000000.index\t\tvocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m69SqKt2yej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bd89fd0a-87d6-4fb1-9e1e-68d8bf8c8a6d"
      },
      "source": [
        "!ls bert_base/"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t     bert_model.ckpt.index  pytorch_model.bin\n",
            "bert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta   vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5-E-QmoeGPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "from transformers import BertModel, BertConfig, BertTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQfw8bVFtzfU",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNRmPMnGtyU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a09da933-7c43-4b5b-d898-cd4c172d4bb0"
      },
      "source": [
        "text = \"CanSino Biologics, headquartered in Tianjin, is close to testing its\" \\\n",
        "      \"novel coronavirus vaccine in a clinical trial in China. CanSino’s approach \" \\\n",
        "      \"involves taking a snippet of coronavirus’ genetic code and entwining it \" \\\n",
        "      \"with a harmless virus, thereby exposing healthy volunteers to the novel \" \\\n",
        "      \"infection and spurring the production of antibodies. The company said \" \\\n",
        "      \"this week that Chinese authorities approved its planned trial, which will \" \\\n",
        "      \"begin as soon as possible. CanSino markets a vaccine for Ebola virus in China.\" \n",
        "\n",
        "bert_base_tokenizer = BertTokenizer.from_pretrained(\"./biobert/vocab.txt\")\n",
        "\n",
        "# 1. Add the special tokens\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# 2. Split the sentence into tokens\n",
        "tokenized_text = bert_base_tokenizer.tokenize(marked_text)\n",
        "\n",
        "# 3. Map the token strings to their vocabulary indeces\n",
        "indexed_tokens = bert_base_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# 4. Display the words with their indices.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
        "\n",
        "# 5. Segment IDs: 0 for sentence 1 and 1 for sentence 2\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "# 6. convert the lists to tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "cans         20,668\n",
            "##ino         4,559\n",
            "bio          25,128\n",
            "##log        13,791\n",
            "##ics         4,724\n",
            ",               117\n",
            "headquartered  9,514\n",
            "in            1,107\n",
            "t               189\n",
            "##ian         1,811\n",
            "##jin        11,966\n",
            ",               117\n",
            "is            1,110\n",
            "close         1,601\n",
            "to            1,106\n",
            "testing       5,193\n",
            "its           1,157\n",
            "##nov        15,847\n",
            "##el          1,883\n",
            "co            1,884\n",
            "##rona       15,789\n",
            "##virus      27,608\n",
            "vaccine      20,034\n",
            "in            1,107\n",
            "a               170\n",
            "clinical      7,300\n",
            "trial         3,443\n",
            "in            1,107\n",
            "chin          5,144\n",
            "##a           1,161\n",
            ".               119\n",
            "cans         20,668\n",
            "##ino         4,559\n",
            "’               787\n",
            "s               188\n",
            "approach      3,136\n",
            "involves      6,808\n",
            "taking        1,781\n",
            "a               170\n",
            "s               188\n",
            "##ni          2,605\n",
            "##ppe        20,564\n",
            "##t           1,204\n",
            "of            1,104\n",
            "co            1,884\n",
            "##rona       15,789\n",
            "##virus      27,608\n",
            "’               787\n",
            "genetic       7,434\n",
            "code          3,463\n",
            "and           1,105\n",
            "en            4,035\n",
            "##t           1,204\n",
            "##win         7,445\n",
            "##ing         1,158\n",
            "it            1,122\n",
            "with          1,114\n",
            "a               170\n",
            "harmless     22,134\n",
            "virus         7,942\n",
            ",               117\n",
            "thereby       8,267\n",
            "exposing     15,952\n",
            "healthy       8,071\n",
            "volunteers    8,118\n",
            "to            1,106\n",
            "the           1,103\n",
            "novel         2,281\n",
            "infection     8,974\n",
            "and           1,105\n",
            "spur         16,650\n",
            "##ring        3,384\n",
            "the           1,103\n",
            "production    1,707\n",
            "of            1,104\n",
            "antibodies   26,491\n",
            ".               119\n",
            "the           1,103\n",
            "company       1,419\n",
            "said          1,163\n",
            "this          1,142\n",
            "week          1,989\n",
            "that          1,115\n",
            "chin          5,144\n",
            "##ese         6,420\n",
            "authorities   3,912\n",
            "approved      4,092\n",
            "its           1,157\n",
            "planned       2,919\n",
            "trial         3,443\n",
            ",               117\n",
            "which         1,134\n",
            "will          1,209\n",
            "begin         3,295\n",
            "as            1,112\n",
            "soon          1,770\n",
            "as            1,112\n",
            "possible      1,936\n",
            ".               119\n",
            "cans         20,668\n",
            "##ino         4,559\n",
            "markets       5,809\n",
            "a               170\n",
            "vaccine      20,034\n",
            "for           1,111\n",
            "e               174\n",
            "##bol        15,792\n",
            "##a           1,161\n",
            "virus         7,942\n",
            "in            1,107\n",
            "chin          5,144\n",
            "##a           1,161\n",
            ".               119\n",
            "[SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni3C5ZWU5wuS",
        "colab_type": "text"
      },
      "source": [
        "## Load weights from default transformers (to compare with the tensorflow model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPRoszqj573v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  embeddings, cls = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "transformer_embeddings = embeddings.squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_slwTZX6Bkx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab168f35-c7c0-43be-a3cd-9975fa2bd8b2"
      },
      "source": [
        "for word, embedding in zip(tokenized_text, transformer_embeddings):\n",
        "  print(word, transformer_embeddings[:5])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "cans tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##ino tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "bio tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##log tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##ics tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            ", tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "headquartered tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "in tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "t tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##ian tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##jin tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            ", tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "is tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "close tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "to tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "testing tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "its tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##nov tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##el tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "co tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##rona tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##virus tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "vaccine tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "in tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "a tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "clinical tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "trial tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "in tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "chin tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##a tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            ". tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "cans tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##ino tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "’ tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "s tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "approach tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "involves tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "taking tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "a tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "s tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##ni tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##ppe tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##t tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "of tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "co tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##rona tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##virus tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "’ tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "genetic tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "code tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "and tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "en tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##t tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##win tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##ing tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "it tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "with tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "a tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "harmless tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "virus tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            ", tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "thereby tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "exposing tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "healthy tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "volunteers tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "to tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "the tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "novel tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "infection tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "and tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "spur tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##ring tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "the tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "production tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "of tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "antibodies tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            ". tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "the tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "company tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "said tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "this tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "week tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "that tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "chin tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##ese tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "authorities tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "approved tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "its tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "planned tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "trial tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            ", tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "which tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "will tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "begin tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "as tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "soon tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "as tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "possible tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            ". tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "cans tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##ino tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "markets tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "a tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "vaccine tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "for tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "e tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##bol tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##a tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "virus tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "in tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "chin tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "##a tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            ". tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n",
            "[SEP] tensor([[ 7.0802e-01, -1.3570e-02, -3.0544e-01,  ..., -3.9950e-01,\n",
            "          3.9953e-01, -3.8603e-04],\n",
            "        [ 2.9583e-01, -3.1500e-01,  2.5154e-01,  ..., -4.2627e-01,\n",
            "          3.0456e-01,  9.8179e-02],\n",
            "        [ 4.2225e-01, -4.2436e-01,  3.2933e-01,  ..., -4.2615e-01,\n",
            "         -2.9616e-01,  2.5861e-02],\n",
            "        [ 6.1493e-01,  5.1981e-02, -1.0528e-01,  ..., -2.2775e-01,\n",
            "          9.2993e-01,  1.8184e-01],\n",
            "        [ 9.7992e-01, -4.0849e-01, -8.9345e-01,  ..., -4.0943e-01,\n",
            "          5.7542e-01, -8.6916e-02]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlfiAwdWuzYh",
        "colab_type": "text"
      },
      "source": [
        "## Load weights from PyTorch converted Tensorflow weights (to compare with Transformers model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJGa7IjOpe0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "configuration = BertConfig.from_json_file('./bert_base/bert_config.json')\n",
        "model = BertModel.from_pretrained(\"./bert_base/pytorch_model.bin\", config=configuration)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  embeddings, cls = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "tensorflow_embeddings = embeddings.squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6B01fpk5nup",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0fee4362-78d5-48d8-898f-2b89f7bba67f"
      },
      "source": [
        "for word, embedding in zip(tokenized_text, tensorflow_embeddings):\n",
        "  print(word, embedding[:5])"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] tensor([ 0.7080, -0.0136, -0.3054, -0.2832, -0.4734])\n",
            "cans tensor([ 0.2958, -0.3150,  0.2515, -0.1083, -0.6709])\n",
            "##ino tensor([ 0.4222, -0.4244,  0.3293,  0.5395,  0.2592])\n",
            "bio tensor([ 0.6149,  0.0520, -0.1053,  0.5756,  0.1492])\n",
            "##log tensor([ 0.9799, -0.4085, -0.8934,  0.3584, -0.4468])\n",
            "##ics tensor([ 0.2792, -0.5877,  0.0782,  0.5845,  0.0549])\n",
            ", tensor([ 0.3582,  0.2693, -0.1371,  0.7319,  0.1760])\n",
            "headquartered tensor([ 0.0457, -0.7216,  0.0814,  0.3653, -0.0360])\n",
            "in tensor([ 0.2864, -0.1087, -0.0292, -0.0799,  0.4864])\n",
            "t tensor([ 0.1461, -0.1209, -0.0017,  0.1688,  0.0571])\n",
            "##ian tensor([-0.3160, -0.4995,  0.0915,  0.5655,  0.4278])\n",
            "##jin tensor([-0.4187, -0.3286,  0.1854,  0.5316,  0.2759])\n",
            ", tensor([0.8008, 0.5355, 0.1528, 0.4869, 0.7257])\n",
            "is tensor([0.0882, 0.0598, 0.1472, 0.6301, 0.3900])\n",
            "close tensor([ 0.0576, -0.0209,  0.1033,  0.2792,  0.0769])\n",
            "to tensor([ 0.1725, -0.1233, -0.2260,  0.3368,  0.2081])\n",
            "testing tensor([ 0.3918, -0.1397,  0.1864,  0.1211, -0.3515])\n",
            "its tensor([ 0.2532, -0.1083,  0.1174,  0.3340, -0.4195])\n",
            "##nov tensor([ 0.5816,  0.1649, -0.0969,  0.1024, -1.1940])\n",
            "##el tensor([ 0.4025, -0.3059,  0.1538,  0.1265, -0.4603])\n",
            "co tensor([1.7296, 0.1538, 0.1735, 0.0712, 0.5699])\n",
            "##rona tensor([ 0.2625, -0.6939, -0.2964, -0.2594, -0.2604])\n",
            "##virus tensor([ 0.2564,  0.0712,  0.6434,  0.4996, -0.2204])\n",
            "vaccine tensor([ 0.6348, -0.1810,  0.0111,  0.4281, -0.2124])\n",
            "in tensor([ 6.4452e-01, -1.6274e-04,  3.6954e-01, -7.6782e-02, -1.3084e-01])\n",
            "a tensor([ 0.5605,  0.1247,  0.1307,  0.1246, -0.2013])\n",
            "clinical tensor([ 0.6438, -0.5443, -0.2650,  0.7778, -0.2478])\n",
            "trial tensor([ 0.9632, -0.0950,  0.0126,  0.4016, -0.5104])\n",
            "in tensor([ 0.6683, -0.1231,  0.0229,  0.1596, -0.1289])\n",
            "chin tensor([-0.0014,  0.5273,  0.0874,  0.9313, -0.4158])\n",
            "##a tensor([-0.0308, -0.4678, -0.4291,  0.8270,  0.1299])\n",
            ". tensor([ 0.9579, -0.1032, -0.5776,  0.6605, -0.3440])\n",
            "cans tensor([-0.4389,  0.7552, -0.1824,  0.5413, -0.3728])\n",
            "##ino tensor([ 0.5161, -0.4126,  0.2307,  0.5058,  0.2258])\n",
            "’ tensor([ 0.1411,  0.0300, -0.3031,  0.5312, -0.4487])\n",
            "s tensor([ 0.0993, -0.2016,  0.4577,  0.3758, -0.1099])\n",
            "approach tensor([ 0.0304, -0.2669,  0.2237,  0.3537, -0.3760])\n",
            "involves tensor([-0.0402, -0.3564,  0.1560,  0.2915,  0.4044])\n",
            "taking tensor([ 0.7426, -0.2205, -0.1313,  0.0629,  0.2542])\n",
            "a tensor([ 0.5845, -0.0596, -0.2014, -0.2205,  0.1146])\n",
            "s tensor([ 0.6119, -0.1833,  0.1213, -0.3479, -0.1032])\n",
            "##ni tensor([ 0.0815, -0.0436, -0.2705, -0.1261, -0.1009])\n",
            "##ppe tensor([ 0.8624, -0.0054, -0.0300, -0.3829, -0.7568])\n",
            "##t tensor([ 0.4021, -0.0939, -0.1261, -0.3382, -0.0860])\n",
            "of tensor([ 0.4191, -0.0025, -0.0111,  0.0118, -0.0246])\n",
            "co tensor([ 1.4756, -0.0145,  0.1012, -0.2734,  0.5497])\n",
            "##rona tensor([ 0.1178, -0.6904, -0.2755, -0.1840, -0.1212])\n",
            "##virus tensor([ 0.0754,  0.0225,  0.6092,  0.3901, -0.2690])\n",
            "’ tensor([ 0.5967, -0.3316,  0.1322,  0.0318, -0.1955])\n",
            "genetic tensor([ 0.8999,  0.2030,  0.1910,  0.7168, -0.1483])\n",
            "code tensor([ 0.8020, -0.1411,  0.2913, -0.2077,  0.0956])\n",
            "and tensor([ 0.4412, -0.7457, -0.4525,  0.0016,  0.9144])\n",
            "en tensor([ 1.7962,  0.5185, -0.2731,  0.4435,  0.3339])\n",
            "##t tensor([ 0.3084,  0.8729, -0.8118,  0.2118,  0.2238])\n",
            "##win tensor([ 1.3792,  0.1312, -1.5702,  0.2170,  0.2092])\n",
            "##ing tensor([ 0.6339,  0.0998, -0.5156,  0.4725,  0.3581])\n",
            "it tensor([ 0.6404, -0.2381, -0.2812, -0.0895,  0.7029])\n",
            "with tensor([ 0.4034,  0.3360, -0.5076,  0.5593,  0.1441])\n",
            "a tensor([ 0.3213,  0.0262, -0.3623, -0.0638,  0.1290])\n",
            "harmless tensor([ 0.6107, -0.0242, -0.0451, -0.0205, -0.3875])\n",
            "virus tensor([ 0.5283, -0.0978,  0.6443,  0.0432, -0.1251])\n",
            ", tensor([0.4002, 0.3737, 0.0844, 0.4130, 0.3833])\n",
            "thereby tensor([ 0.4355, -0.1569, -0.0952,  0.1053,  0.4640])\n",
            "exposing tensor([ 0.5913, -0.2876,  0.1530,  0.2772, -0.3934])\n",
            "healthy tensor([ 0.2937, -0.3321, -0.0115,  0.0660, -0.5963])\n",
            "volunteers tensor([-0.2176, -0.1604,  0.2907,  0.2767, -0.5523])\n",
            "to tensor([9.0107e-01, 2.9038e-01, 1.0255e-01, 5.1679e-04, 8.1720e-01])\n",
            "the tensor([ 0.4455, -0.0261, -0.0405,  0.3142,  0.4935])\n",
            "novel tensor([ 0.6471,  0.2416,  0.0118, -0.0099, -0.3735])\n",
            "infection tensor([0.5606, 0.0837, 0.5277, 0.4735, 0.0490])\n",
            "and tensor([ 0.3927, -0.3728, -0.3963, -0.1439,  0.4659])\n",
            "spur tensor([ 0.2762,  0.0221,  0.2319, -0.2521, -0.1009])\n",
            "##ring tensor([-0.2151,  0.1196,  0.2655,  0.2723, -0.2035])\n",
            "the tensor([ 0.0873,  0.3858,  0.1763,  0.1992, -0.0656])\n",
            "production tensor([ 0.4062,  0.1688, -0.1090,  0.0295, -0.3284])\n",
            "of tensor([ 0.6856,  0.3898, -0.3313,  0.3190,  0.2135])\n",
            "antibodies tensor([ 0.5278,  0.2867,  0.2142,  0.2673, -0.1663])\n",
            ". tensor([ 0.9730, -0.1097, -0.5739,  0.7036, -0.3036])\n",
            "the tensor([-0.0109, -0.7832, -0.4027,  0.2862, -0.0621])\n",
            "company tensor([ 0.4287, -0.4061,  0.1881,  0.1724,  0.0729])\n",
            "said tensor([-0.1195, -0.3082,  0.0387, -0.2709, -0.0126])\n",
            "this tensor([-0.3541, -0.5216,  0.5107, -0.0221,  0.4026])\n",
            "week tensor([-0.1461, -0.4881, -0.0232, -0.0222, -0.0730])\n",
            "that tensor([-0.1561, -0.1487,  0.0014, -0.0200,  0.5609])\n",
            "chin tensor([ 0.0676,  0.7298, -0.0511,  1.1519, -0.2920])\n",
            "##ese tensor([ 0.1466, -0.5562, -0.0243,  0.5833, -0.3200])\n",
            "authorities tensor([-0.1790, -0.6070,  0.1699,  0.5138, -0.1774])\n",
            "approved tensor([ 0.2949, -0.4381,  0.3761,  0.3726,  0.0005])\n",
            "its tensor([ 0.3351, -0.0881, -0.3469,  0.2620,  0.1616])\n",
            "planned tensor([-0.2287,  0.6928, -0.1552,  0.2332, -0.2060])\n",
            "trial tensor([ 0.6168,  0.3257, -0.1737,  0.1962, -0.6714])\n",
            ", tensor([ 0.3571,  0.8102,  0.2270,  0.3376, -0.1427])\n",
            "which tensor([-0.2030,  0.2334, -0.1496, -0.0676,  0.6095])\n",
            "will tensor([-0.0345,  0.1779, -0.2477,  0.0057,  0.4432])\n",
            "begin tensor([ 0.3496, -0.0213,  0.2676, -0.1751,  0.0926])\n",
            "as tensor([-0.6429, -0.0884, -0.1965, -0.3733,  0.2010])\n",
            "soon tensor([-0.3858,  0.7628,  0.6937, -0.1906,  0.6282])\n",
            "as tensor([ 0.6923, -0.7812,  0.1783, -0.5511, -0.5694])\n",
            "possible tensor([0.8241, 0.0915, 0.3128, 0.1775, 0.5248])\n",
            ". tensor([-0.2711,  0.0935, -0.0370,  0.4081, -0.2706])\n",
            "cans tensor([-0.4203,  0.6854, -0.2643,  0.5081, -0.4037])\n",
            "##ino tensor([ 0.3760, -0.4102,  0.1660,  0.6014,  0.3494])\n",
            "markets tensor([ 0.7673, -0.1082,  0.3397,  0.1251, -0.3316])\n",
            "a tensor([ 0.4197, -0.0687, -0.0453,  0.0491, -0.3339])\n",
            "vaccine tensor([ 0.6989, -0.1937, -0.2654,  0.4020, -0.4643])\n",
            "for tensor([ 0.2793, -0.2761, -0.6686,  0.3249, -0.2995])\n",
            "e tensor([ 0.4101,  0.0121, -0.4067,  0.0608,  0.3205])\n",
            "##bol tensor([-0.4479, -0.6046, -0.3411, -0.5050, -0.0495])\n",
            "##a tensor([-0.3351, -0.4588, -0.2534,  0.0575,  0.5541])\n",
            "virus tensor([0.2865, 0.0093, 0.4955, 0.2769, 0.1131])\n",
            "in tensor([ 0.3787, -0.1109, -0.0005, -0.0825,  0.3212])\n",
            "chin tensor([-0.0243,  0.8984,  0.0690,  0.7172, -0.2622])\n",
            "##a tensor([-0.0709, -0.3743, -0.5175,  0.8396,  0.3793])\n",
            ". tensor([ 0.3971, -0.0045, -0.0201,  0.0319,  0.1104])\n",
            "[SEP] tensor([ 0.9049, -0.1235, -0.5513,  0.6464, -0.2914])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4gLSToXvBH7",
        "colab_type": "text"
      },
      "source": [
        "## Load weights BioBERT\n",
        "Since I have used `transformers-cli convert` to convert the tensorflow model to PyTorch, the rest of the program works just like transformers. Forget Tensorflow even existed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnxdre-zp1b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "configuration = BertConfig.from_json_file('./biobert/bert_config.json')\n",
        "model = BertModel.from_pretrained(\"./biobert/pytorch_model.bin\", config=configuration)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  embeddings, cls = model(tokens_tensor, segments_tensors)\n",
        "biobert_embeddings = embeddings.squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuXUNoNtvr8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d3cfaea-a92c-4372-cad8-89d1dc6a3a30"
      },
      "source": [
        "for word, embedding in zip(tokenized_text, biobert_embeddings):\n",
        "  print(word, embedding[:5])"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] tensor([-0.0065,  0.1404, -0.4794, -0.0418, -0.4687])\n",
            "cans tensor([ 0.0781, -0.0577, -0.1690,  0.2767,  0.0975])\n",
            "##ino tensor([ 0.2606, -0.4994, -0.1520,  0.0454,  0.0726])\n",
            "bio tensor([ 0.3948, -0.0728, -0.2140,  0.1543,  0.1730])\n",
            "##log tensor([ 0.3731, -0.2170, -0.7370,  0.2493, -0.4223])\n",
            "##ics tensor([-0.2962,  0.2090, -0.0406,  1.0999,  0.2371])\n",
            ", tensor([-0.3337, -0.1598, -0.4558,  0.6887,  0.5007])\n",
            "headquartered tensor([-0.0473, -0.8199, -0.8299,  0.6298, -0.6146])\n",
            "in tensor([ 0.4575, -0.5467, -0.3971,  0.4630,  0.1766])\n",
            "t tensor([ 1.7152e-01, -4.4227e-01, -2.3838e-01,  2.4482e-04,  5.0961e-01])\n",
            "##ian tensor([-0.3334, -0.3899,  0.0256,  0.6071,  0.1383])\n",
            "##jin tensor([-0.7429, -0.3694,  0.2580,  0.2781,  0.5911])\n",
            ", tensor([-0.3528,  0.1087, -0.3621,  0.6264,  0.4396])\n",
            "is tensor([-0.4351, -0.0294, -0.0987,  0.8041, -0.0696])\n",
            "close tensor([-0.3168,  0.1785, -0.2560,  0.0392, -0.1784])\n",
            "to tensor([ 0.3835,  0.1897, -0.4827, -0.1329,  0.3971])\n",
            "testing tensor([ 0.2824, -0.1037,  0.0814, -0.1260, -0.2508])\n",
            "its tensor([-0.1773,  0.0442,  0.1706,  0.2310,  0.0587])\n",
            "##nov tensor([ 0.4057,  0.4335,  0.2049, -0.3412, -0.6457])\n",
            "##el tensor([ 0.4563,  0.0181,  0.0963, -0.2385, -0.5217])\n",
            "co tensor([ 0.3094, -0.1247, -0.2793, -0.5519,  0.1104])\n",
            "##rona tensor([ 0.2918, -0.0864, -0.2318, -0.5725, -0.3842])\n",
            "##virus tensor([-0.2853,  0.3370, -0.0682,  0.2251, -0.5034])\n",
            "vaccine tensor([ 0.0182, -0.0279, -0.2174,  0.3268, -0.3309])\n",
            "in tensor([ 0.4014, -0.1258,  0.1287, -0.4592,  0.0029])\n",
            "a tensor([ 0.2008, -0.0981, -0.5293,  0.0161, -0.0441])\n",
            "clinical tensor([-0.1160, -0.0705, -0.5629,  0.2152, -0.3503])\n",
            "trial tensor([ 0.3821, -0.0942, -0.6904,  0.1169, -0.5017])\n",
            "in tensor([ 0.5277, -0.5185,  0.1240,  0.1709, -0.4672])\n",
            "chin tensor([-0.4287,  0.3501,  0.4166, -0.0468,  0.0532])\n",
            "##a tensor([-0.4153, -1.0163,  0.1206,  0.5490, -0.0553])\n",
            ". tensor([-0.1826, -0.0665,  0.1133,  0.1962,  0.1074])\n",
            "cans tensor([-0.1303,  0.2462, -0.2544,  0.3733,  0.1640])\n",
            "##ino tensor([ 0.1602, -0.2900, -0.3937, -0.0895,  0.2813])\n",
            "’ tensor([-0.0390,  0.0988, -0.4594,  0.3562, -0.0855])\n",
            "s tensor([-0.5780, -0.2315,  0.0014,  0.5293,  0.2817])\n",
            "approach tensor([-0.2463,  0.0537, -0.0611,  0.5343,  0.0817])\n",
            "involves tensor([-0.4321, -0.3450,  0.0540,  0.4991,  0.7964])\n",
            "taking tensor([-0.1001, -0.1665, -0.1480, -0.0707, -0.0432])\n",
            "a tensor([ 0.1697, -0.4529, -0.4822,  0.2332,  0.0481])\n",
            "s tensor([ 0.0489,  0.3177,  0.2201, -0.0373,  0.1482])\n",
            "##ni tensor([-0.2836, -0.8284, -0.1652, -0.0053,  0.6154])\n",
            "##ppe tensor([-0.0402, -0.0018,  0.3998,  0.2267, -0.0682])\n",
            "##t tensor([-0.1118, -0.1346, -0.1047,  0.2615,  0.3466])\n",
            "of tensor([ 0.0285, -0.1974, -0.3149,  0.1042,  0.4502])\n",
            "co tensor([ 0.4878, -0.3339, -0.4231, -0.4015,  0.1603])\n",
            "##rona tensor([ 0.4118, -0.2001, -0.3492, -0.5001, -0.3665])\n",
            "##virus tensor([-0.2241,  0.1902, -0.3048,  0.0784, -0.4677])\n",
            "’ tensor([-0.0265, -0.3475, -0.1039, -0.1160, -0.3305])\n",
            "genetic tensor([ 0.2620,  0.3539,  0.2882, -0.1037, -0.0171])\n",
            "code tensor([ 0.1095, -0.0195,  0.1395, -0.5003,  0.1670])\n",
            "and tensor([ 0.3850, -0.6645, -0.9024, -0.0888,  0.2063])\n",
            "en tensor([ 2.9452e-01, -2.9837e-01, -2.1192e-06,  1.6083e-01,  3.4949e-01])\n",
            "##t tensor([-0.1436, -0.5607,  0.0041,  0.0202,  1.1023])\n",
            "##win tensor([-0.0782, -0.1371, -0.9367, -0.5014,  0.0583])\n",
            "##ing tensor([-0.1954, -0.5788, -0.5686, -0.2569,  0.5371])\n",
            "it tensor([-0.1613,  0.2701, -0.4964, -0.1995, -0.0083])\n",
            "with tensor([ 0.4671,  0.1479, -0.3824, -0.6030,  0.0542])\n",
            "a tensor([ 0.3887, -0.0105, -0.4588, -0.4030, -0.0592])\n",
            "harmless tensor([ 0.1555,  0.0370, -0.2275, -0.4146, -0.4335])\n",
            "virus tensor([-0.0354, -0.2174,  0.1209, -0.1202, -0.3359])\n",
            ", tensor([-0.0255,  0.0811, -0.1078,  0.1694, -0.0067])\n",
            "thereby tensor([ 0.5263, -0.5577, -0.2451,  0.3724,  0.0427])\n",
            "exposing tensor([ 0.5742, -0.5665,  0.4602, -0.6151, -0.1704])\n",
            "healthy tensor([-0.0664, -0.2883,  0.0291,  0.1463, -0.4207])\n",
            "volunteers tensor([-0.4808, -0.3217,  0.3173,  0.0839, -0.4599])\n",
            "to tensor([ 0.7907, -0.1776,  0.2733, -0.5975,  0.2928])\n",
            "the tensor([-0.1253, -0.1593,  0.0835, -0.0859,  0.0229])\n",
            "novel tensor([ 0.1641, -0.0041,  0.0029, -0.2364, -0.3719])\n",
            "infection tensor([ 0.2533, -0.0846,  0.2262,  0.1193, -0.3388])\n",
            "and tensor([ 0.3342, -0.4443, -0.3556, -0.2328,  0.2821])\n",
            "spur tensor([ 0.2396, -0.1352,  0.1895, -0.6033, -0.1054])\n",
            "##ring tensor([-0.1882, -0.3655,  0.2213, -0.4549,  0.1068])\n",
            "the tensor([-0.4633, -0.8114,  0.3553, -0.1824,  0.4507])\n",
            "production tensor([-0.2737, -0.2344, -0.1655, -0.5720, -0.3625])\n",
            "of tensor([-0.0979, -0.4863, -0.6124,  0.0479, -0.0676])\n",
            "antibodies tensor([ 0.0276, -0.3192,  0.0444,  0.4272, -0.4715])\n",
            ". tensor([-0.1488, -0.0440,  0.3960,  0.1216, -0.0882])\n",
            "the tensor([-0.3904, -0.3289, -0.1937,  0.4034,  0.2406])\n",
            "company tensor([-0.4892, -0.3146, -0.0300,  0.3711, -0.0390])\n",
            "said tensor([-0.4067,  0.1268, -0.1730,  0.3351, -0.2901])\n",
            "this tensor([-0.6750, -0.5283, -0.0536,  0.1826, -0.2383])\n",
            "week tensor([-0.4582, -0.3037, -0.1306,  0.0951, -0.2150])\n",
            "that tensor([ 0.3245,  0.2115, -0.3035, -0.0808,  0.5696])\n",
            "chin tensor([-0.4330,  0.4747,  0.0224, -0.0008,  0.1420])\n",
            "##ese tensor([-0.2851, -0.3988,  0.2807,  0.4349,  0.0453])\n",
            "authorities tensor([-0.5261, -0.2932, -0.3839,  0.5424, -0.1622])\n",
            "approved tensor([ 0.4063,  0.1044, -0.4863,  0.3827,  0.0678])\n",
            "its tensor([-0.3927, -0.0247, -0.5532,  0.2582,  0.1019])\n",
            "planned tensor([-0.1400,  0.4932, -0.6281,  0.1482, -0.2643])\n",
            "trial tensor([-0.0543,  0.1302, -0.8909, -0.1769, -0.5643])\n",
            ", tensor([ 0.0745,  0.5410, -0.3370,  0.3671,  0.4167])\n",
            "which tensor([-0.5861,  0.1145, -0.2462,  0.3070,  0.0980])\n",
            "will tensor([ 0.1607,  0.2298, -0.6278,  0.4611, -0.1484])\n",
            "begin tensor([ 0.3139, -0.0577, -0.4239,  0.2114,  0.1205])\n",
            "as tensor([-0.6978,  0.0438, -0.0635, -0.0732, -0.2523])\n",
            "soon tensor([-0.3406,  0.3198,  0.1627, -0.3195, -0.2593])\n",
            "as tensor([ 0.5177, -0.9334, -0.1166,  0.0025,  0.2985])\n",
            "possible tensor([ 0.4926,  0.3055, -0.4613,  0.5006,  0.2399])\n",
            ". tensor([-0.2104, -0.2050,  0.2107,  0.1019, -0.1268])\n",
            "cans tensor([-0.1199, -0.0012, -0.3632,  0.3335,  0.1871])\n",
            "##ino tensor([ 0.0999, -0.3217, -0.5351,  0.1239,  0.2026])\n",
            "markets tensor([ 0.2189,  0.2088,  0.0026,  0.4492, -0.5094])\n",
            "a tensor([-0.1061, -0.1788, -0.2703,  0.4355, -0.3650])\n",
            "vaccine tensor([-0.0272, -0.2304, -0.5360,  0.1937, -0.8460])\n",
            "for tensor([ 0.1034, -0.1741, -0.3623, -0.0977, -0.8353])\n",
            "e tensor([ 0.3057, -0.3762, -0.5311,  0.1086,  0.2390])\n",
            "##bol tensor([ 0.4331, -0.1114, -0.3313, -0.8643, -0.3851])\n",
            "##a tensor([-0.1714, -0.9422, -0.3782,  0.0145, -0.4944])\n",
            "virus tensor([ 0.0265, -0.1093, -0.2592, -0.0930, -0.4874])\n",
            "in tensor([ 0.3417, -0.4425,  0.1141, -0.0149, -0.2198])\n",
            "chin tensor([-0.3666,  0.1973,  0.2031, -0.2521, -0.0496])\n",
            "##a tensor([-0.2650, -0.8252,  0.0627,  0.4710, -0.1315])\n",
            ". tensor([-0.3653, -0.3758, -0.0464,  0.3105, -0.1987])\n",
            "[SEP] tensor([ 0.2954, -0.4622, -1.2800, -0.0686, -0.1937])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3KbN8oA7raw",
        "colab_type": "text"
      },
      "source": [
        "### Proof that the transformer and tensorflow models are exactly the same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKdUzt1f4wh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "38b16920-66c6-4144-ea39-c05c9cec1b31"
      },
      "source": [
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "cos(transformer_embeddings, tensorflow_embeddings)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOz_OcpG7K9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3017e809-467f-47d9-c066-b1ee9e2c34d7"
      },
      "source": [
        "### Proof that BioBERT is different\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "cos(transformer_embeddings, biobert_embeddings)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5491, 0.2367, 0.7055, 0.7289, 0.6874, 0.7433, 0.7235, 0.6245, 0.7102,\n",
              "        0.5944, 0.5964, 0.7147, 0.7552, 0.7649, 0.7119, 0.6661, 0.8036, 0.7680,\n",
              "        0.6295, 0.6644, 0.6835, 0.6668, 0.7080, 0.7909, 0.7922, 0.7764, 0.7392,\n",
              "        0.7532, 0.7173, 0.6286, 0.6636, 0.1458, 0.6408, 0.6956, 0.4937, 0.7039,\n",
              "        0.7931, 0.7793, 0.7573, 0.7381, 0.6466, 0.5461, 0.5848, 0.6744, 0.7274,\n",
              "        0.7239, 0.6702, 0.7473, 0.6605, 0.7595, 0.7963, 0.7404, 0.6165, 0.5975,\n",
              "        0.6010, 0.6864, 0.7585, 0.7158, 0.7924, 0.7962, 0.7774, 0.7682, 0.7141,\n",
              "        0.7472, 0.7796, 0.7659, 0.6960, 0.7697, 0.7322, 0.7723, 0.7588, 0.6802,\n",
              "        0.6828, 0.7248, 0.7513, 0.7290, 0.7716, 0.1386, 0.7247, 0.7461, 0.6725,\n",
              "        0.6522, 0.6458, 0.6806, 0.6918, 0.7519, 0.7046, 0.7458, 0.7460, 0.7713,\n",
              "        0.7560, 0.7307, 0.6832, 0.6655, 0.6582, 0.6282, 0.6579, 0.5215, 0.5910,\n",
              "        0.7567, 0.6379, 0.7164, 0.7278, 0.7646, 0.7769, 0.7234, 0.6913, 0.5411,\n",
              "        0.6376, 0.7092, 0.7445, 0.6391, 0.6445, 0.7807, 0.4255])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTKsHfoe75Wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}